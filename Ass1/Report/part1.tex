\chapter{Fundamental Abstractions - S3}

\section{High level overview of S3}
We would use the following addressing scheme for our memory
abstraction: \texttt{(ip, memory address)}. So, to access memory, one
would request a memory address from a specific IP, and this would be
trivially translated to the machine with the memory by separating the
two parts of the address. For simplicity, we represent the address as
a tuple.

We have no centralised component in this design. Each client just
sends requests directly to the relevant machine. This system is widely
scalable, limited only by the IP addressing used. So, using IPv6, we
can scale up to $2^{128}$ machines, each with an addressable memory
address space of e.g. $2^{64}$. Of course current networking or memory
hardware will never support using the entire address space, but then
again neither will our bank accounts.

If a machine is unavailable, it will not respond, and that is
interpretted as an error, which is passed to the caller. But we have
no redundancy, so if a server is down, the memory it stores is
unavailable as well.


\section{Pseudocode of S3 APIs}
Figure \ref{pseudocode} shows the pseudocode of our API to read/write
directly from/to the memory server with the desired memory. As can be
seen, the IP address is needed to read or write memory. This is
obtained when allocating memory. Memory allocation is done by
broadcasting a request to allocate said memory to all memory servers,
and accepting the server that responds the fastest. This server then
returns the full address \texttt{(IP, address)} of the newly allocated
memory, and with that information we can read and write the data.

A problem with our setup is that in case there is a network failure
during a write, we have no way knowing whether the write was commited
or not.

\begin{figure}
\begin{verbatim}
def read((ip, address, size)):
    send(ip, serialize({action: 'GET', address: address, size: size}))
    res = recv(ip, timeout = 5000)
    if res == None:
        return False
    return res

def write(ip, address, size, data):
    send(ip, serialize({action: 'SET', address: address, size: size, data: data}))
    res = recv(ip, timeout = 5000)
    if res == None:
        return False
    return True
\end{verbatim}
\caption{Pseudocode for read and write API \label{pseudocode}}
\end{figure}

\section{Read/write atomicity in S3}
Yes, each single call to read/write is atomic in itself, as long as we
assume that the server implementing the API will commit each memory
write atomically. On write failures, we will not know, whether the
write was committed or not, but in either case the memory will be in a
consistent state.

For a given chain of calls, they are not atomic and cannot be atomic
without extending the API to communicate when to commit the changes.

We want atomicity because it ensures that a request was either
completed successfully or completely aborted. This way, we always know
whether something has been read/written or not. It is not likely to
cause significant performance overhead and with all else being equal
is a nice feature to have.

\section{Dynamic aspects of S3}
Our design requires that the system has at least 1 memory machine to
be usable. Other than that, there are no assumptions or requirements
as to the number of machines in the system. Machines can join and
leave the network independently as they please. A strategy to allocate
memory could be either a limited broadcast or trying random hosts.

However, if a machine leaves the network while having active memory
allocations, these memory allocations will be lost (the assignment
specifies to not consider redundancy to mask failures). If we were to
support memory access in spite of server leaves, we would introduce
redundancy to the system. For example, instead of using single memory
servers per IP, each IP could point to a cluster of memory servers
that share the same memory. This would give us redundancy
corresponding to the number of servers in the cluster.

If a more dynamic approach is desired, then DHT such as Kademlia is
probably a better solution.
